\documentclass{article}
\usepackage{cancel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[authoryear]{natbib}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[left=3cm,right=3cm]{geometry}
\usepackage{float}
\usepackage{graphicx} % Required for inserting images
\usepackage{tikz}
\usepackage{multicol}
\newcommand{\revin}{\mathbin{\rotatebox[origin=c]{180}{$\in$}}}
\usetikzlibrary{fit}
\newcommand{\overlay}[2][]{\tikz[overlay,
  remember picture, #1]{#2}}
\tikzset{
  highlighted/.style = { draw, thick, rectangle,
                         rounded corners, inner sep = 0pt,
                         fill = red!15, fill opacity = 0.5
                       }
}
\newcommand{\highlight}[1]{%
  \overlay{
    \node [fit = (left.north west) (right.south east),
           highlighted] (#1) {}; }
}
\newcommand{\flag}[2]{\overlay[baseline=(#1.base)]
  {\node (#1) {$#2$};}}

% TODO
% implementing in python the kronecker operator
% also the vec operator
% try to replicate fact1
% checking the np-complete and np-hard problems
% 

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Autoencoders explained}
\author{Lorenzo Bozzoni}
\date{\today}

\begin{document}

\maketitle

This small article is a theoretical project for the Numerical Analyisis for Machine Learning course at Politecnico di Milano. In particular, using as main reference \citep{pmlr-v27-baldi12a}, we are going to explore the topic of autoencoders; starting from the linear case, we then proceed to the boolean case and concluding with the clustering discussion and overcomplete autoencoders.  

including linear and boolean class and a final part on the complexity.

\tableofcontents

\section{Introduction}
Autoencoders are simple learning circuits which aim to transform inputs into outputs with
the least possible amount of distortion. To derive a fairly general framework, an $n/p/n$  autoencoder is defined by a t-uple $n,p,m,\mathbb{F},\mathbb{G},\mathcal{A},\mathcal{B},\mathcal{X},\mathcal{Y}, \Delta$, where:
\begin{itemize}
    \item $\mathbb{F}$ and $\mathbb{G}$ are sets
    \item $n$ and $p$ are positive integers
    \item $\mathcal{A}$ is a class of functions from $\mathbb{G}^p$ to $\mathbb{F}^n$
    \item $\mathcal{B}$ is a class of functions from $\mathbb{F}^n$ to $\mathbb{G}^p$
    \item $\mathcal{X} = {x_1, \dots, x_m}$ is a set of $m$ (training) vectors in $\mathbb{F}^n$. When the external targets are present, we let $\mathcal{Y} = {y_1, \dots, y_m}$ denote the corresponding set of $m$ target vectors in $\mathbb{F}^n$ 
    \item $\Delta$ is a dissimilarity or distortion function defined over $\mathbb{F}^n$
\end{itemize}
For any $A \in \mathcal{A}$ and $B \in \mathcal{B}$, the autoencoder transforms an input vector $x \in \mathbb{F}^n$ into an output vector $A \circ B(x) \in \mathbb{F}^n$.
\[
\mathcal{X} \revin x = 
\begin{bmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n
\end{bmatrix}
\overset{B}{\longrightarrow}
\begin{bmatrix}
    z_1\\
    z_2\\
    \vdots\\
    z_p
\end{bmatrix}
\overset{A}{\longrightarrow}
\begin{bmatrix}
    y_1\\
    y_2\\
    \vdots\\
    y_n
\end{bmatrix}
= y \in \mathcal{Y}
\]
 The corresponding \textbf{autoencoder problem} is to find $A \in \mathcal{A}$ and $B \in \mathcal{B}$ that minimize the overall distortion function:
\begin{equation}
    \min E(A,B) = \min_{A ,B} \sum_{t=1}^m E(A,B)=\min_{A ,B} \sum_{t=1}^m \Delta(x_t, A \circ B(x_t))
\end{equation}
In the non auto-associative case, when external targets $y_t$ are provided, the minimization
problem becomes:
\begin{equation}
    \min E(A,B) = \min_{A ,B} \sum_{t=1}^m E(A,B)=\min_{A ,B} \sum_{t=1}^m \Delta(y_t, A \circ B(x_t))
\end{equation}
It is important to notice that if $p < n$ corresponds to a compression or feature extraction, while $p > n$ corresponds to a decompression. Here is a simple classification of the different types of autoencoders:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{./Images/Autoencoders_Classification.png}
  \caption{Simple classification of autoencoders}
\end{figure}
We are going to explore linear real and boolean autoencoders in the following sections.



\input{LinearAutoencoders}
\input{BooleanAutoencoders.tex}
\input{ClusteringComplexity.tex}
\input{OvercompleteAutoencoders.tex}


\newpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{plainnat}    %unsrt
\nocite{*}
\bibliography{references.bib}
\end{document}
