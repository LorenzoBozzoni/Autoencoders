
@InProceedings{pmlr-v27-baldi12a,
  title = 	 {Autoencoders, Unsupervised Learning, and Deep Architectures},
  author = 	 {Baldi, Pierre},
  booktitle = 	 {Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
  pages = 	 {37--49},
  year = 	 {2012},
  editor = 	 {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
  volume = 	 {27},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bellevue, Washington, USA},
  month = 	 {02 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf},
  url = 	 {https://proceedings.mlr.press/v27/baldi12a.html},
  abstract = 	 {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory.}
}


@article{BALDI198953,
title = {Neural networks and principal component analysis: Learning from examples without local minima},
journal = {Neural Networks},
volume = {2},
number = {1},
pages = {53-58},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90014-2},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900142},
author = {Pierre Baldi and Kurt Hornik},
keywords = {Neural networks, Principal component analysis, Learning, Back propagation},
abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.}
}

@article{harary1988cubical,
  title={Cubical graphs and cubical dimensions},
  author={Harary, Frank},
  journal={Computers \& Mathematics with Applications},
  volume={15},
  number={4},
  pages={271--275},
  year={1988},
  publisher={Elsevier}
}

@article{hartman1976homeomorphic,
  title={The homeomorphic embedding of Kn in the m-cube},
  author={Hartman, Jehuda},
  journal={Discrete Mathematics},
  volume={16},
  number={2},
  pages={157--160},
  year={1976},
  publisher={Elsevier}
}

@article{afrati1985complexity,
  title={The complexity of cubical graphs},
  author={Afrati, Foto and Papadimitriou, Christos H and Papageorgiou, George},
  journal={Information and control},
  volume={66},
  number={1-2},
  pages={53--60},
  year={1985},
  publisher={Elsevier}
}

@article{livingston1988embeddings,
  title={Embeddings in hypercubes},
  author={Livingston, Marilynn and Stout, Quentin F},
  journal={Mathematical and Computer Modelling},
  volume={11},
  pages={222--227},
  year={1988},
  publisher={Elsevier}
}


@article{havel1972b,
  title={$ B $-valuations of graphs},
  author={Havel, Ivan and Mor{\'a}vek, Jaroslav},
  journal={Czechoslovak Mathematical Journal},
  volume={22},
  number={2},
  pages={338--351},
  year={1972},
  publisher={Institute of Mathematics, Academy of Sciences of the Czech Republic}
}

@article{megiddo1984complexity,
  title={On the complexity of some common geometric location problems},
  author={Megiddo, Nimrod and Supowit, Kenneth J},
  journal={SIAM journal on computing},
  volume={13},
  number={1},
  pages={182--196},
  year={1984},
  publisher={SIAM}
}

@article{winkler1983proof,
  title={Proof of the squashed cube conjecture},
  author={Winkler, Peter M},
  journal={Combinatorica},
  volume={3},
  pages={135--139},
  year={1983},
  publisher={Springer}
}

@article{mahajan2012planar,
  title={The planar k-means problem is NP-hard},
  author={Mahajan, Meena and Nimbhorkar, Prajakta and Varadarajan, Kasturi},
  journal={Theoretical Computer Science},
  volume={442},
  pages={13--21},
  year={2012},
  publisher={Elsevier}
}

@article{vattani2009hardness,
  title={The hardness of k-means clustering in the plane},
  author={Vattani, Andrea},
  journal={Manuscript, accessible at http://cseweb. ucsd. edu/avattani/papers/kmeans\_hardness. pdf},
  volume={617},
  year={2009}
}

@misc{garey1979computers,
  title={Computers and intractability. W. H},
  author={Garey, Michael R and Johnson, David S},
  year={1979},
  publisher={Freeman and Co., San Francisco, Calif}
}
