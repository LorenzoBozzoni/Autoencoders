\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{crane}
\usepackage{smartdiagram}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{graphicx} % Required for inserting images
%\usepackage[T1]{fontenc}

\smartdiagramset{border color=black,
set color list={cyan!50!cyan, cyan!50!cyan, cyan!50!cyan, cyan!50!cyan, cyan!50!cyan, cyan!50!cyan},    
back arrow disabled=true}


\usetikzlibrary{fit}
\newcommand{\revin}{\mathbin{\rotatebox[origin=c]{180}{$\in$}}}
\newcommand{\overlay}[2][]{\tikz[overlay,
  remember picture, #1]{#2}}
\tikzset{
  highlighted/.style = { draw, thick, rectangle,
                         rounded corners, inner sep = 0pt,
                         fill = red!15, fill opacity = 0.5
                       }
}
\newcommand{\highlight}[1]{%
  \overlay{
    \node [fit = (left.north west) (right.south east),
           highlighted] (#1) {}; }
}
\newcommand{\flag}[2]{\overlay[baseline=(#1.base)]
  {\node (#1) {$#2$};}}

\title{Autoencoders}
\author{Lorenzo Bozzoni}
\institute{Politecnico di Milano}
\date{September 2024}   

\begin{document}

\frame{\titlepage}



\begin{frame}{Table of content}
    \tableofcontents
\end{frame}


\section{General framework}
\begin{frame}{General framework}
The general framework of autoencoders is:
\[
\mathcal{X} \revin x = 
\begin{bmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n
\end{bmatrix}
\overset{B}{\longrightarrow}
\begin{bmatrix}
    z_1\\
    z_2\\
    \vdots\\
    z_p
\end{bmatrix}
\overset{A}{\longrightarrow}
\begin{bmatrix}
    y_1\\
    y_2\\
    \vdots\\
    y_n
\end{bmatrix}
= y \in \mathcal{Y}
\]
Where $B \in \mathcal{B}$ which is a set of functions from $\mathbb{F}^n$ to $\mathbb{G}^p$ while $A \in \mathcal{A}$ which is a set of functions from $\mathbb{G}^p$ to $\mathbb{F}^n$.
\end{frame}

\begin{frame}{General framework}
The goal is to find a pair of functions $A,B$ such that the generic dissimilarity function $\Delta$ is minimized:
\[
\min E(A,B) = \min_{A,B} \sum_{t=1}^m E(x_t,y_t) = \min_{A,B} \sum_{t=1}^m \Delta(A \circ B(x_t),y_t)  
\]
In the auto-associative case the right side of the autoencoder is again $x_t$.

\textbf{The focus is not on the reconstruction of the input but rather on how well we can compress the input data in the hidden layer without losing information.}
\end{frame}


\section{Linear autoencoders}
\begin{frame}{Linear autoencoders}
In the case of \textbf{linear autoencoders} we have:
\begin{itemize}
    \item $\mathbb{F},\mathbb{G}$ are fields
    \item $\mathcal{A},\mathcal{B}$ are the classes of linear transformations: $A,B$ are respectively matrices of shape $p \times n$ and $n \times p$
    \item $\Delta$ is the squared Euclidean distance ($L_2^2$ norm)
\end{itemize}
\end{frame}


\begin{frame}{Linear autoencoders}
    In general the problem of finding the matrices $A,B$ that minimize the error function $E$ is a non-convex optimization problem.\\
    \vspace{0.3cm}
    However, fixing one of the two matrices, the problem becomes convex so \textbf{we can find the optimal value by alternating the optimization of the two matrices.}
    Fixing $A$ the optimal $B$ is: 
    \[
        B = \hat{B}(A) = (A^\intercal A)^{-1} A^\intercal 
    \]
    While fixing $B$ the optimal $A$ is:
    \[
        A = \hat{A}(B) = \Sigma_{XX}B^\intercal (B\Sigma_{XX}B^\intercal)^{-1}
    \]
    Where $\Sigma_{XX}$ is the covariance matrix of the input data.
\end{frame}

\begin{frame}{Linear autoencoders}
    \begin{itemize}
        \item Since we are applying only linear transformations, the best compression we can achieve is the one that projects the input data on the subspace spanned by the eigenvectors of the covariance matrix of the input data.\\

        \item This corresponds to the \textbf{Principal Component Analysis (PCA)} when the input is normalized as follows:
        \[
            \hat{x}_{i,j} = \dfrac{1}{\sqrt{m}}\left(x_{ij} - \dfrac{1}{m}\sum_{k=1}^m x_{kj}\right)
        \]    
    \end{itemize}
    
\end{frame}

\section{Boolean autoencoders}
\begin{frame}{Boolean autoencoders}
    In the case of \textbf{Boolean autoencoders} we have:
    \begin{itemize}
        \item $\mathbb{F},\mathbb{G}$ are the Boolean fields, i.e $\{0,1\}$, the Galois field $\mathbb{F}_2$
        \item $\mathcal{A},\mathcal{B}$ are the classes of Boolean transformations: $A,B$ are respectively matrices of shape $p \times n$ and $n \times p$ with entries in $\{0,1\}$
        \item $\Delta$ is the Hamming distance
    \end{itemize}
\end{frame}

\begin{frame}{Boolean autoencoders}
We start defining the following lemma:
\begin{lemma}
    The vector \text{Majority}(p) is a vector in $\mathbb{H}^n$ closest to the center of gravity of the vectors $p_1, \dots, p_k$ and it minimizes the function $E(q) = \sum_{i=1}^k \Delta(p_i, q)$.
\end{lemma}
Where the center of gravity is the vector $c$ in $\mathbb{R}^n$ with coordinates 
\[
    c_j = \dfrac{\left(\sum\limits_{i=1}^{k} p_{ji}\right)}{k}
\]
So, each $c_j$ is the average of the $j$-th components of the vectors $p_1, \dots, p_k$.
For any $j$, $(p)_j$ is the closest binary value to $c_j$.
\end{frame}



\begin{frame}{Boolean autoencoders}
This means that for each row, we check the majority of the values and we set the value of the row to the majority value.\\
\vspace{1cm}
    \[
    k\text{-rows }
  \begin{bmatrix}
    \flag{left}{1} & 0  & 1 & 1 & 1 & \flag{right}{0}\\
    1 & 1 & 0 & 1 & 0  & 1 \\
    0 & 0 & 1 & 0 & 1  & 0\\
    1 & 1 & 0 & 1 & 1  & 0 \\
  \end{bmatrix}
  \hspace{3cm}
  \highlight{N}
  \flag{left}{1}
    \flag{right}{} 
  \highlight{NT}
\]
\overlay{
  \draw[->, thick, red, dashed] (N) -- (NT)
    node [pos=0.48, above] {$p_1$};
  \node[above of = N ] { $n$-element rows   };
  \node[below of = NT] { $Majority(p_1)$ };
}

\end{frame}


\begin{frame}{Boolean autoencoders}
    A \textbf{Voronoi partition} of $\mathbb{H}^n$ generated by the vectors $p_1, \dots, p_k$ is a partition of $\mathbb{H}^n$ into $k$ regions $\mathcal{C}^{Vor}(p_1), \dots, \mathcal{C}^{Vor}(p_k)$ such that for each $x$ in $\mathbb{H}^n$:
\[
    x \in \mathcal{C}^{Vor}(p_i) \iff \Delta(x, p_i) \leq \Delta(x, p_j) \text{ for all } j \neq i    
\]
\begin{multicols}{2}
    \begin{center}
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.3\textwidth]{./Images/Euclidean_Voronoi_diagram.png}
            \caption{Euclidean distance}
        \end{figure}
    \end{center}
    \begin{center}
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.3\textwidth]{./Images/Manhattan_Voronoi_Diagram.png}
            \caption{Manhattan distance}
        \end{figure}
    \end{center}
\end{multicols}
\end{frame}


\begin{frame}{Boolean autoencoders}
Considering the two steps mapping:
\[
    x \xrightarrow{B} h \xrightarrow{A} y
\]
\hspace{0.5cm}
\begin{theorem}
    \textbf{Fixed layer solution}: if the $A$ mapping is fixed , then the optimal mapping $B^*$ is given by $B^*(x) = h_i$ for any $x$ in $\mathcal{C}_i = \mathcal{C}^{Vor}(A(h_i))$. Conversely, if $B$ is fixed, then the optimal mapping $A^*$ is given by $A^*(h_i) = \text{Majority}\left[\mathcal{X} \cap B^{-1}(h_i)\right]$ 
\end{theorem}
\end{frame}


\begin{frame}{Boolean autoencoders}
    \begin{multicols}{2}
        If we consider the input-output layers to have a cardinality of 4 and the hidden layer to have a cardinality of 2, then this would mean that there would be $2^2 = 4$ centroids given by the $A$ mapping in the space $\mathbb{H}^4$ showed in figure:

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\textwidth]{./Images/Hypercube4Dimensions.png}
            %\caption{Hypercube in 4 dimensions}
        \end{figure}
    \end{multicols}
    So, the Voronoi partition would be the partition of the hypercube into 4 regions using as metric the Hamming distance, i.e. the number of edges that need to be crossed to go from one point to each centroid.

\end{frame}

\section{Clustering complexity}
\begin{frame}{Complexity recap}
The basic classes for complexity are:
\begin{itemize}
    \item $\mathcal{P}$ is the class of problems that can be \textit{solved} in polynomial time by a deterministic TM
    \item $\mathcal{NP}$ is the class of problems for which a solution can be \textit{verified} in polynomial time by a deterministic TM. The class $\mathcal{NP}$ is the class of problems that can be solved non-deterministically in polynomial time
    \begin{itemize}
        \item \textbf{$\mathcal{NP}$-complete} if it is in $\mathcal{NP}$ and every problem in $\mathcal{NP}$ can be reduced to it in polynomial time
        \item \textbf{$\mathcal{NP}$-hard} if there is a $\mathcal{NP}$-complete problem that can be reduced to it in polynomial time
    \end{itemize}
\end{itemize}
\end{frame}




\begin{frame}{Complexity recap}
    \begin{theorem}
        Consider the following hypercube clustering problem:
        \begin{itemize}
            \item \textbf{Input:} $m$ binary vectors $x_1, \dots, x_m$ of length $n$ and an integer $k$
            \item \textbf{Output:} $k$ binary vectors $c_1, \dots, c_k$ of length $n$ (the centroids) and a function $f$ from $\{x_1, \dots, x_m\}$ to $\{c_1, \dots, c_k\}$ that minimizes the distortion 
            \[
                E = \sum_{t=1}^m \Delta(x_t, f(x_t))    
            \]
            Where $\Delta$ is Hamming distance.
        \end{itemize}
        The hypercube clustering decision problem $\mathcal{NP}$-hard when $k \sim m^\epsilon$ $(\epsilon > 0)$
    \end{theorem}
\end{frame}



\begin{frame}{Clustering complexity}
To prove the hypercube clustering problem is $\mathcal{NP}$-hard we need to reduce prove that an $\mathcal{NP}$-complete problem can be reduced to it in polynomial time. The following reductions are used:
\begin{center}
    \smartdiagram[flow diagram:horizontal]{3-SAT, $\mathbb{R}^2$ clustering, hypercube\ clustering}
\end{center}
\end{frame}



\end{document}
